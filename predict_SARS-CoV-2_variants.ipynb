{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "23b1bbd0",
   "metadata": {},
   "source": [
    "# Predicting SARS-CoV-2 Variants - SVM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "fdb37b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing modules\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from src.scripts.preprocessing import preprocess\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.svm import SVC\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e6c797bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data\n",
    "predict_data = preprocess(\"data/test_set\").spectra2df()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1dbc1d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feature matrix\n",
    "X_predict = predict_data.drop([\"Class\", \"Sample\"], axis=1)\n",
    "\n",
    "y = predict_data[\"Class\"]\n",
    "\n",
    "# Scaling Feature matrix\n",
    "sds = joblib.load(\"models/scaler.pkl\") # Loading scaler params\n",
    "scaled_X_predict = pd.DataFrame(sds.transform(X_predict), columns=X_predict.columns)\n",
    "\n",
    "# Loading SVM model\n",
    "model = joblib.load(\"models/SVM_model.pkl\")\n",
    "\n",
    "# Model prediction\n",
    "y_pred = model.predict(scaled_X_predict.to_numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3a450d5f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted SARS-CoV-2 Variants: ['Omicron' 'Kappa']\n"
     ]
    }
   ],
   "source": [
    "print(f'Predicted SARS-CoV-2 Variants: {y_pred}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5858c340",
   "metadata": {},
   "source": [
    "# Predicting SARS-CoV-2 Variants - BiLSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "35a64623",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing modules\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import torch.optim as optim\n",
    "from sklearn.preprocessing import LabelEncoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2e616222",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label Encoder\n",
    "label_encoder = LabelEncoder()\n",
    "label_encoder = label_encoder.fit(['Wildtype', 'Kappa', 'Omicron', 'Delta'])\n",
    "\n",
    "# Reshaping\n",
    "scaled_X_predict = scaled_X_predict.to_numpy()\n",
    "scaled_X_predict = scaled_X_predict.reshape(scaled_X_predict.shape[0], 1, scaled_X_predict.shape[1])\n",
    "\n",
    "# Tensor\n",
    "X_predict_tensor = torch.tensor(scaled_X_predict, dtype=torch.float32)\n",
    "y_predict_tensor = torch.tensor(label_encoder.transform(predict_data[\"Class\"]), dtype=torch.long)\n",
    "\n",
    "# Dataloader\n",
    "batch_size = 16\n",
    "predict_loader = DataLoader(TensorDataset(X_predict_tensor, y_predict_tensor), batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cb259f09",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BiLSTMClassifier(\n",
      "  (lstm): LSTM(1400, 256, num_layers=3, batch_first=True, dropout=0.3, bidirectional=True)\n",
      "  (attention): Attention(\n",
      "    (attn_weights): Linear(in_features=512, out_features=1, bias=True)\n",
      "  )\n",
      "  (layer_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
      "  (dropout): Dropout(p=0.3, inplace=False)\n",
      "  (fc): Linear(in_features=512, out_features=4, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Attention\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, hidden_size):\n",
    "        super(Attention, self).__init__()\n",
    "        self.attn_weights = nn.Linear(hidden_size, 1)\n",
    "\n",
    "    def forward(self, lstm_output):\n",
    "        attn_scores = self.attn_weights(lstm_output).squeeze(-1)\n",
    "        attn_weights = torch.softmax(attn_scores, dim=1)\n",
    "        context = torch.sum(lstm_output * attn_weights.unsqueeze(-1), dim=1)\n",
    "        return context\n",
    "\n",
    "# BiLSTM Model\n",
    "class BiLSTMClassifier(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, num_layers, num_classes, dropout_prob=0.3):\n",
    "        super(BiLSTMClassifier, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, num_layers, batch_first=True, \n",
    "                            dropout=dropout_prob, bidirectional=True)\n",
    "        self.attention = Attention(hidden_size * 2)\n",
    "        self.layer_norm = nn.LayerNorm(hidden_size * 2)\n",
    "        self.dropout = nn.Dropout(dropout_prob)\n",
    "        self.fc = nn.Linear(hidden_size * 2, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = self.attention(out)\n",
    "        out = self.layer_norm(out)\n",
    "        out = self.dropout(out)\n",
    "        out = self.fc(out)\n",
    "        return out\n",
    "\n",
    "# Model (BiLSTM Model with attention mechanism)\n",
    "model = BiLSTMClassifier(1400, 256, 3, len(label_encoder.classes_))\n",
    "print(model)\n",
    "\n",
    "# Loss function\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Optimizer\n",
    "optimizer = optim.AdamW(model.parameters(), lr=1e-4, weight_decay=1e-4)\n",
    "\n",
    "# Learning rate scheduler\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "719b7b81",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading BiLSTM model\n",
    "model = torch.load(\"models/BiLSTM_model.pth\") \n",
    "\n",
    "# Model prediction\n",
    "model.eval()\n",
    "y_test, y_pred = [], []\n",
    "with torch.no_grad():\n",
    "    for X_batch, y_batch in predict_loader:\n",
    "        X_batch, y_batch = X_batch.to(\"cpu\"), y_batch.to(\"cpu\")\n",
    "        outputs = model(X_batch)\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        y_test.extend(y_batch.cpu().numpy())\n",
    "        y_pred.extend(predicted.cpu().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "69acdf1f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted SARS-CoV-2 Variants: ['Kappa', 'Kappa']\n"
     ]
    }
   ],
   "source": [
    "print(f'Predicted SARS-CoV-2 Variants: {list(label_encoder.inverse_transform(y_pred))}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
